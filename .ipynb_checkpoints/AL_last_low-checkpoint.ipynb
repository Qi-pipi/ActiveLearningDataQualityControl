{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48aac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import matthews_corrcoef,cohen_kappa_score,balanced_accuracy_score\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from modAL.models import ActiveLearner, Committee\n",
    "from modAL.uncertainty import uncertainty_sampling,margin_sampling,entropy_sampling\n",
    "from functools import partial\n",
    "from modAL.batch import uncertainty_batch_sampling\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def compute ratio\n",
    "def comp_ratio(data):\n",
    "    instance = data[(data['label']==1)]\n",
    "    rate=len(instance)/len(data)*100\n",
    "    print(rate)\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f5d127",
   "metadata": {},
   "source": [
    "# Splite Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1495332",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=pd.read_csv('./firsttrain/6903102.csv')\n",
    "test_set=pd.read_csv('./lasttest/6903102.csv')\n",
    "len(train_set),len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set2,test_set2=getLastSplit(data,0.3,dirty_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a296cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_ratio(train_set)\n",
    "comp_ratio(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27927c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function use random grid search to find best parameters\n",
    "def getPar(model,dist,data,niter):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    x,y=data.shape\n",
    "\n",
    "    clf = model\n",
    "\n",
    "    param_dist = dist\n",
    "    grid = RandomizedSearchCV(clf,param_dist,cv = 3,scoring = \"balanced_accuracy\",n_iter=niter,n_jobs = -1)\n",
    "\n",
    "    #train\n",
    "    grid.fit(data.iloc[:,0:y-1],data.iloc[:,y-1])\n",
    "    #get best parameter\n",
    "    print(grid.best_score_)\n",
    "    return grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12137e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric\n",
    "def computeMetric(y_tru,y_pre):\n",
    "    acc = accuracy_score(y_tru,y_pre)\n",
    "    pre=precision_score(y_tru,y_pre)\n",
    "    recall=recall_score(y_tru,y_pre)\n",
    "    cm=confusion_matrix(y_tru,y_pre)\n",
    "    f1 = f1_score(y_tru,y_pre)\n",
    "    mcc=matthews_corrcoef(y_tru, y_pre)\n",
    "    kappa=cohen_kappa_score(y_tru, y_pre)\n",
    "    bac=balanced_accuracy_score(y_tru,y_pre)\n",
    "    print(\"acc:\",acc)\n",
    "    print(\"balanced acc:\",bac)\n",
    "    print(\"precision:\",pre)\n",
    "    print(\"recall:\",recall)\n",
    "    print(\"cm:\",cm)\n",
    "    print(\"f1:\",f1)\n",
    "    print(\"MCC:\", mcc)\n",
    "    print(\"Kappa:\",kappa)\n",
    "    \n",
    "    # confusion matrix\n",
    "#     cmap1 = sns.diverging_palette(260,-10,s=50, l=75, n=5, as_cmap=True)\n",
    "#     plt.subplots(figsize=(12,8))\n",
    "#     cf_matrix = confusion_matrix(y_tru, y_pre)\n",
    "#     sns.heatmap(cf_matrix/np.sum(cf_matrix), cmap = cmap1, annot = True, annot_kws = {'size':15})\n",
    "    \n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbc8df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(performance_history):\n",
    "    fig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\n",
    "    ax.plot(performance_history)\n",
    "    ax.scatter(range(len(performance_history)), performance_history, s=13)\n",
    "    ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=5, integer=True))\n",
    "    ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=10))\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n",
    "    ax.set_ylim(bottom=0, top=1)\n",
    "    ax.grid(True)\n",
    "    ax.set_title('Incremental classification accuracy')\n",
    "    ax.set_xlabel('Query iteration')\n",
    "    ax.set_ylabel('Classification Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695922d",
   "metadata": {},
   "source": [
    "# splite train(pool), test, Labeled, Unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fca32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=train_set.shape\n",
    "#pool \n",
    "X_Pool = train_set.iloc[:,0:y-1].values\n",
    "y_Pool = train_set.iloc[:,y-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set\n",
    "X_test=test_set.iloc[:,0:y-1]\n",
    "y_tru=test_set.iloc[:,y-1]\n",
    "print(len(X_test),len(y_tru))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc3675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pool\n",
    "def initial_data(n_initial,X_Pool,y_Pool):\n",
    "    #inital\n",
    "    initial_idx = np.random.choice(range(len(X_Pool)), size=n_initial, replace=False)\n",
    "    #initial Labeled data\n",
    "    #X_initial, y_initial = X_train[], y_train[initial_idx]\n",
    "    X_L = X_Pool[initial_idx]\n",
    "    y_L = y_Pool[initial_idx]\n",
    "    # Unlabeled data\n",
    "    # X_U = Pool_X[ini_num:]\n",
    "    # y_U = Pool_X[ini_num:]\n",
    "    X_U, y_U = np.delete(X_Pool, initial_idx, axis=0), np.delete(y_Pool, initial_idx, axis=0)\n",
    "    return X_L,y_L,X_U,y_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set RNG seed for reproducibility.\n",
    "RANDOM_STATE_SEED = 123\n",
    "np.random.seed(RANDOM_STATE_SEED)\n",
    "\n",
    "#n_initial= 100\n",
    "n_initial = 1000\n",
    "N_QUERIES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfa452",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_initial,y_initial,X_re,y_re=initial_data(n_initial,X_Pool,y_Pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e4d53",
   "metadata": {},
   "source": [
    "# molAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(classifier, X_pool):\n",
    "    n_samples = len(X_pool)\n",
    "    query_idx = np.random.choice(range(n_samples))\n",
    "    return query_idx, X_pool[query_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deeebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def al(clf,strategy,X_L,y_L):\n",
    "\n",
    "#         learner = ActiveLearner(estimator=clf,\n",
    "#                                 query_strategy=random_sampling,\n",
    "#                                 X_training=X_L, y_training=y_L)\n",
    "#     else:\n",
    "    learner = ActiveLearner(estimator=clf,\n",
    "                            query_strategy=strategy,\n",
    "                            X_training=X_L, y_training=y_L)\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def al_learn(clf,sampling,X_initial,y_initial,X_re,y_re):\n",
    "    X_L = X_initial.copy()\n",
    "    y_L = y_initial.copy()\n",
    "    X_U,y_U =X_re.copy(),y_re.copy()\n",
    "    #print(len(X_U),len(y_U))\n",
    "    learner = al(clf,sampling,X_L,y_L)\n",
    "    y_pre=learner.predict(X_test)\n",
    "    unqueried_score=cohen_kappa_score(y_tru, y_pre)\n",
    "    #unqueried_score= f1_score(y_tru,y_pre)\n",
    "    print(\"unqueried --------------------->\",unqueried_score)\n",
    "    performance_history = [unqueried_score]\n",
    "    # Query\n",
    "    for index in range(N_QUERIES):\n",
    "        query_index, query_instance = learner.query(X_U)\n",
    "        # Teach ActiveLearner model the record it has requested.\n",
    "        X, y = X_U[query_index].reshape(1, -1), y_U[query_index].reshape(1, )\n",
    "        print(index+1,\"query label --------------------->\",y)\n",
    "        learner.teach(X=X, y=y)\n",
    "        \n",
    "        X_U, y_U = np.delete(X_U, query_index, axis=0), np.delete(y_U, query_index)\n",
    "        y_pre=learner.predict(X_test)\n",
    "        kappa=cohen_kappa_score(y_tru, y_pre)\n",
    "        #f1=f1_score(y_tru,y_pre)\n",
    "        print(index+1,\"-------------------->\",kappa)\n",
    "        #print(index+1,\"-------------------->\",f1)\n",
    "        # Recall precision F1\n",
    "        performance_history.append(kappa)\n",
    "        #performance_history.append(f1)\n",
    "        \n",
    "    #print(len(X_U))\n",
    "    #draw(performance_history)\n",
    "    return performance_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcae8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf1 KNN\n",
    "clf1 = KNeighborsClassifier()\n",
    "dic1={'n_neighbors':[1,2,3,4,5,6,7,8]}\n",
    "para1=getPar(clf1,dic1,test_set,10)\n",
    "clf1 = KNeighborsClassifier(n_neighbors=para1['n_neighbors'],n_jobs=-1)\n",
    "clf1.fit(X_Pool, y_Pool)\n",
    "y_pre=clf1.predict(X_test)\n",
    "computeMetric(y_tru,y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e140237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf2 lightgbm\n",
    "clf2 = lgb.LGBMClassifier()\n",
    "\n",
    "dic2 = {'learning_rate' : [0.01, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1, 0.2, 0.3, 0.4],\n",
    "              'n_estimators' : [100, 200, 300, 400, 500, 600, 800, 900, 1000, 1500, 2000],\n",
    "              'num_leaves': range(6, 50), \n",
    "              'min_child_samples': range(10, 200, 10), \n",
    "              'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "              'max_depth': [-1, 1, 2, 3, 4, 5, 6, 7],\n",
    "              'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "              'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "para2=getPar(clf2,dic2,test_set,10)\n",
    "\n",
    "    \n",
    "clf2 = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=para2['num_leaves'], max_depth=-1, \n",
    "                              learning_rate=para2['learning_rate'], n_estimators=para2['n_estimators'], \n",
    "                              subsample_for_bin=200000, \n",
    "                              objective=None, class_weight=None, min_split_gain=0.0, \n",
    "                              min_child_weight=para2['min_child_weight'],\n",
    "                              min_child_samples=para2['min_child_samples'], \n",
    "                              subsample=1.0, \n",
    "                              subsample_freq=0, colsample_bytree=1.0, reg_alpha=para2['reg_alpha'], \n",
    "                              reg_lambda=para2['reg_lambda'], random_state=None, n_jobs=-1, importance_type='split')\n",
    "clf2.fit(X_Pool, y_Pool)\n",
    "y_pre=clf2.predict(X_test)\n",
    "computeMetric(y_tru,y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd03466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf3 gradientBoostingClassifier\n",
    "clf3 = lgb.LGBMClassifier()\n",
    "\n",
    "dic3 = {'learning_rate' : [0.01, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1, 0.2, 0.3, 0.4],\n",
    "              'n_estimators' : [100, 200, 300, 400, 500, 600, 800, 900, 1000, 1500, 2000],\n",
    "              'num_leaves': range(6, 50), \n",
    "              'min_child_samples': range(10, 200, 10), \n",
    "              'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "              'max_depth': [-1, 1, 2, 3, 4, 5, 6, 7],\n",
    "              'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "              'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "para3=getPar(clf3,dic3,test_set,10)\n",
    "\n",
    "    \n",
    "clf3 = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=para3['num_leaves'], max_depth=-1, \n",
    "                              learning_rate=para3['learning_rate'], n_estimators=para3['n_estimators'], \n",
    "                              subsample_for_bin=200000, \n",
    "                              objective=None, class_weight=None, min_split_gain=0.0, \n",
    "                              min_child_weight=para3['min_child_weight'],\n",
    "                              min_child_samples=para3['min_child_samples'], \n",
    "                              subsample=1.0, \n",
    "                              subsample_freq=0, colsample_bytree=1.0, reg_alpha=para3['reg_alpha'], \n",
    "                              reg_lambda=para3['reg_lambda'], random_state=None, n_jobs=-1, importance_type='split')\n",
    "clf3.fit(X_Pool, y_Pool)\n",
    "y_pre=clf3.predict(X_test)\n",
    "computeMetric(y_tru,y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089159a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf4 catboost\n",
    "clf4 = CatBoostClassifier(loss_function='Logloss')\n",
    "# dic4 = {'learning_rate': [0.03, 0.1],\n",
    "#         'depth': [4, 6, 10],\n",
    "#         'l2_leaf_reg': [1, 3, 5, 7, 9]}\n",
    "# #para3=getPar(clf4,dic4,test_set,10)\n",
    "\n",
    "# grid_search_result = clf4.grid_search(dic4, \n",
    "#                                        X=X_Pool, \n",
    "#                                        y=y_Pool)\n",
    "\n",
    "clf4.fit(X_Pool, y_Pool)\n",
    "y_pre=clf4.predict(X_test)\n",
    "computeMetric(y_tru,y_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb1dde",
   "metadata": {},
   "source": [
    "## random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a90cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling=random_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adad705",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric01 = al_learn(clf1,sampling,X_initial,y_initial,X_re,y_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb05c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric02 = al_learn(clf2,sampling,X_initial,y_initial,X_re,y_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39259b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric03 = al_learn(clf3,sampling,X_initial,y_initial,X_re,y_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9845ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric04 = al_learn(clf4,sampling,X_initial,y_initial,X_re,y_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7279b320",
   "metadata": {},
   "source": [
    "## Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d366d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling=uncertainty_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric11 = al_learn(clf1,sampling,X_initial,y_initial,X_re,y_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee690cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric12 = al_learn(clf2,sampling,X_initial,y_initial,X_re,y_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9298210",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric13 = al_learn(clf3,sampling,X_initial,y_initial,X_re,y_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ee527",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric14 = al_learn(clf4,sampling,X_initial,y_initial,X_re,y_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2090ac",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614c6748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_format(data):\n",
    "    metrics_arr=np.transpose(metrics)# array T\n",
    "    df = pd.DataFrame(metrics_arr, columns=['Random_Knn', 'Random_Lighgbm',\"Random_GradientBoosting\",\"Random_Catboost\",\n",
    "                                   'Uncertainty_Knn', 'Uncertainty_Lighgbm',\"Uncertainty_GradientBoosting\",\"Uncertainty_Catboost\",\n",
    "                                   ])\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9331c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=[metric01,metric02,metric03,metric04,\n",
    "        metric11,metric12,metric13,metric14]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e14296",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=con_format(metrics)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs('./result',exist_ok = True) #\n",
    "\n",
    "df.to_csv(\"./result/last_Kappa_6903102.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef80a80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "al",
   "language": "python",
   "name": "al"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
